{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianh\\Desktop\\environments\\mlenv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\tianh\\Desktop\\environments\\mlenv\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.layers as KL\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import backend as KB\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Reshape, Permute, Lambda, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.3\n",
    "LOAD_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "STAMP = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 404290 texts in train.csv\n",
      "Found 85518 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 37391\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 300)      25655700    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 265)          599960      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 530)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 530)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 530)          2120        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 145)          76995       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 145)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 145)          580         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            146         batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 26,335,501\n",
      "Trainable params: 678,451\n",
      "Non-trainable params: 25,657,050\n",
      "__________________________________________________________________________________________________\n",
      "lstm\n",
      "Train on 566006 samples, validate on 242574 samples\n",
      "Epoch 1/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.4395 - acc: 0.6688 - val_loss: 0.3643 - val_acc: 0.6976\n",
      "Epoch 2/50\n",
      "566006/566006 [==============================] - 121s 215us/step - loss: 0.3726 - acc: 0.6956 - val_loss: 0.3481 - val_acc: 0.7102\n",
      "Epoch 3/50\n",
      "566006/566006 [==============================] - 122s 216us/step - loss: 0.3516 - acc: 0.7155 - val_loss: 0.3320 - val_acc: 0.7358\n",
      "Epoch 4/50\n",
      "566006/566006 [==============================] - 122s 216us/step - loss: 0.3354 - acc: 0.7296 - val_loss: 0.3195 - val_acc: 0.7475\n",
      "Epoch 5/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.3230 - acc: 0.7413 - val_loss: 0.3086 - val_acc: 0.7552\n",
      "Epoch 6/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.3132 - acc: 0.7509 - val_loss: 0.3103 - val_acc: 0.7689\n",
      "Epoch 7/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.3048 - acc: 0.7592 - val_loss: 0.2966 - val_acc: 0.7700\n",
      "Epoch 8/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2972 - acc: 0.7668 - val_loss: 0.2924 - val_acc: 0.7814\n",
      "Epoch 9/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2908 - acc: 0.7733 - val_loss: 0.2932 - val_acc: 0.7839\n",
      "Epoch 10/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2849 - acc: 0.7799 - val_loss: 0.2874 - val_acc: 0.7816\n",
      "Epoch 11/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2796 - acc: 0.7844 - val_loss: 0.2858 - val_acc: 0.7958\n",
      "Epoch 12/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2753 - acc: 0.7887 - val_loss: 0.2834 - val_acc: 0.7964\n",
      "Epoch 13/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2715 - acc: 0.7928 - val_loss: 0.2860 - val_acc: 0.8042\n",
      "Epoch 14/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2675 - acc: 0.7967 - val_loss: 0.2847 - val_acc: 0.8043\n",
      "Epoch 15/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2638 - acc: 0.7998 - val_loss: 0.2817 - val_acc: 0.8035\n",
      "Epoch 16/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2609 - acc: 0.8027 - val_loss: 0.2805 - val_acc: 0.8034\n",
      "Epoch 17/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2573 - acc: 0.8062 - val_loss: 0.2834 - val_acc: 0.8087\n",
      "Epoch 18/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2547 - acc: 0.8089 - val_loss: 0.2764 - val_acc: 0.8081\n",
      "Epoch 19/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2513 - acc: 0.8117 - val_loss: 0.2773 - val_acc: 0.8107\n",
      "Epoch 20/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2493 - acc: 0.8140 - val_loss: 0.2748 - val_acc: 0.8081\n",
      "Epoch 21/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2471 - acc: 0.8158 - val_loss: 0.2771 - val_acc: 0.8129\n",
      "Epoch 22/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2448 - acc: 0.8178 - val_loss: 0.2758 - val_acc: 0.8116\n",
      "Epoch 23/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2436 - acc: 0.8189 - val_loss: 0.2772 - val_acc: 0.8122\n",
      "Epoch 24/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2413 - acc: 0.8219 - val_loss: 0.2753 - val_acc: 0.8138\n",
      "Epoch 25/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2395 - acc: 0.8230 - val_loss: 0.2841 - val_acc: 0.8189\n",
      "Epoch 26/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2385 - acc: 0.8243 - val_loss: 0.2749 - val_acc: 0.8131\n",
      "Epoch 27/50\n",
      "566006/566006 [==============================] - 123s 217us/step - loss: 0.2365 - acc: 0.8259 - val_loss: 0.2791 - val_acc: 0.8185\n",
      "Epoch 28/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2353 - acc: 0.8275 - val_loss: 0.2808 - val_acc: 0.8180\n",
      "Epoch 29/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2334 - acc: 0.8287 - val_loss: 0.2702 - val_acc: 0.8133\n",
      "Epoch 30/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2327 - acc: 0.8297 - val_loss: 0.2763 - val_acc: 0.8161\n",
      "Epoch 31/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2319 - acc: 0.8304 - val_loss: 0.2752 - val_acc: 0.8168\n",
      "Epoch 32/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2301 - acc: 0.8321 - val_loss: 0.2795 - val_acc: 0.8211\n",
      "Epoch 33/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2295 - acc: 0.8326 - val_loss: 0.2750 - val_acc: 0.8210\n",
      "Epoch 34/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2275 - acc: 0.8343 - val_loss: 0.2788 - val_acc: 0.8235\n",
      "Epoch 35/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2266 - acc: 0.8351 - val_loss: 0.2787 - val_acc: 0.8218\n",
      "Epoch 36/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2267 - acc: 0.8355 - val_loss: 0.2731 - val_acc: 0.8206\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2250 - acc: 0.8363 - val_loss: 0.2828 - val_acc: 0.8224\n",
      "Epoch 38/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2248 - acc: 0.8370 - val_loss: 0.2737 - val_acc: 0.8194\n",
      "Epoch 39/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2244 - acc: 0.8371 - val_loss: 0.2847 - val_acc: 0.8268\n",
      "Epoch 40/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2233 - acc: 0.8386 - val_loss: 0.2738 - val_acc: 0.8225\n",
      "Epoch 41/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2218 - acc: 0.8402 - val_loss: 0.2813 - val_acc: 0.8244\n",
      "Epoch 42/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2208 - acc: 0.8404 - val_loss: 0.2774 - val_acc: 0.8256\n",
      "Epoch 43/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2203 - acc: 0.8409 - val_loss: 0.2760 - val_acc: 0.8248\n",
      "Epoch 44/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2196 - acc: 0.8419 - val_loss: 0.2792 - val_acc: 0.8262\n",
      "Epoch 45/50\n",
      "566006/566006 [==============================] - 124s 218us/step - loss: 0.2192 - acc: 0.8417 - val_loss: 0.2807 - val_acc: 0.8257\n",
      "Epoch 46/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2182 - acc: 0.8427 - val_loss: 0.2778 - val_acc: 0.8241\n",
      "Epoch 47/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2176 - acc: 0.8437 - val_loss: 0.2774 - val_acc: 0.8257\n",
      "Epoch 48/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2178 - acc: 0.8436 - val_loss: 0.2718 - val_acc: 0.8221\n",
      "Epoch 49/50\n",
      "566006/566006 [==============================] - 124s 219us/step - loss: 0.2169 - acc: 0.8450 - val_loss: 0.2786 - val_acc: 0.8265\n",
      "Epoch 50/50\n",
      "566006/566006 [==============================] - 123s 218us/step - loss: 0.2159 - acc: 0.8451 - val_loss: 0.2819 - val_acc: 0.8277\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP)\n",
    "if LOAD_DATA:\n",
    "    model.load_weights(bst_model_path)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=50, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attention_after_lstm(inputs, SINGLE_ATTENTION_VECTOR=False):\n",
    "    a = Lambda(lambda x: KB.expand_dims(x, axis=1))(inputs)\n",
    "    input_dim = int(a.shape[2])\n",
    "    a = Permute((2, 1))(a)\n",
    "    a = Reshape((input_dim, 1))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(1, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: KB.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    a_probs = Lambda(lambda x: KB.squeeze(x, axis=1))(a_probs)\n",
    "    outputs = KL.multiply([inputs, a_probs])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attention_before_lstm(inputs, T, SINGLE_ATTENTION_VECTOR=True):\n",
    "    a = inputs\n",
    "    input_dim = int(a.shape[2])\n",
    "    a = Permute((2, 1))(a)\n",
    "    a = Reshape((input_dim, T))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(T, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: KB.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    outputs = KL.multiply([inputs, a_probs])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_att_before = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "lstm_layer_att_before = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "sequence_1_input_att_before = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1_att_before = embedding_layer_att_before(sequence_1_input_att_before)\n",
    "x1_att_before = add_attention_before_lstm(embedded_sequences_1_att_before, MAX_SEQUENCE_LENGTH, SINGLE_ATTENTION_VECTOR=True)\n",
    "x1_att_before = lstm_layer_att_before(x1_att_before)\n",
    "sequence_2_input_att_before = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2_att_before = embedding_layer_att_before(sequence_2_input_att_before)\n",
    "y1_att_before = add_attention_before_lstm(embedded_sequences_2_att_before, MAX_SEQUENCE_LENGTH, SINGLE_ATTENTION_VECTOR=True)\n",
    "y1_att_before = lstm_layer_att_before(embedded_sequences_2_att_before)\n",
    "merged_att_before = concatenate([x1_att_before, y1_att_before])\n",
    "merged_att_before = Dropout(rate_drop_dense)(merged_att_before)\n",
    "merged_att_before = BatchNormalization()(merged_att_before)\n",
    "merged_att_before = Dense(num_dense, activation=act)(merged_att_before)\n",
    "merged_att_before = Dropout(rate_drop_dense)(merged_att_before)\n",
    "merged_att_before = BatchNormalization()(merged_att_before)\n",
    "preds_att_before = Dense(1, activation='sigmoid')(merged_att_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 300)      25655700    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 300, 30)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300, 30)      0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300, 30)      930         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 30)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 300, 30)      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 30, 300)      0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 30, 300)      0           embedding_2[0][0]                \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 265)          599960      multiply_1[0][0]                 \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 530)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 530)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 530)          2120        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 145)          76995       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 145)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 145)          580         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            146         batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 26,336,431\n",
      "Trainable params: 679,381\n",
      "Non-trainable params: 25,657,050\n",
      "__________________________________________________________________________________________________\n",
      "Train on 566006 samples, validate on 242574 samples\n",
      "Epoch 1/50\n",
      "566006/566006 [==============================] - 151s 267us/step - loss: 0.4538 - acc: 0.6585 - val_loss: 0.3901 - val_acc: 0.6979\n",
      "Epoch 2/50\n",
      "566006/566006 [==============================] - 173s 306us/step - loss: 0.3881 - acc: 0.6855 - val_loss: 0.3642 - val_acc: 0.6993\n",
      "Epoch 3/50\n",
      "566006/566006 [==============================] - 184s 326us/step - loss: 0.3714 - acc: 0.6985 - val_loss: 0.3593 - val_acc: 0.7187\n",
      "Epoch 4/50\n",
      "566006/566006 [==============================] - 172s 303us/step - loss: 0.3605 - acc: 0.7072 - val_loss: 0.3429 - val_acc: 0.7060\n",
      "Epoch 5/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3517 - acc: 0.7161 - val_loss: 0.3344 - val_acc: 0.7240\n",
      "Epoch 6/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3452 - acc: 0.7222 - val_loss: 0.3322 - val_acc: 0.7377\n",
      "Epoch 7/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3394 - acc: 0.7278 - val_loss: 0.3268 - val_acc: 0.7448\n",
      "Epoch 8/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3349 - acc: 0.7327 - val_loss: 0.3240 - val_acc: 0.7500\n",
      "Epoch 9/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3306 - acc: 0.7369 - val_loss: 0.3177 - val_acc: 0.7496\n",
      "Epoch 10/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3264 - acc: 0.7415 - val_loss: 0.3153 - val_acc: 0.7414\n",
      "Epoch 11/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3234 - acc: 0.7441 - val_loss: 0.3173 - val_acc: 0.7628\n",
      "Epoch 12/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3198 - acc: 0.7482 - val_loss: 0.3111 - val_acc: 0.7603\n",
      "Epoch 13/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3172 - acc: 0.7509 - val_loss: 0.3125 - val_acc: 0.7683\n",
      "Epoch 14/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3142 - acc: 0.7542 - val_loss: 0.3061 - val_acc: 0.7470\n",
      "Epoch 15/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.3114 - acc: 0.7567 - val_loss: 0.3112 - val_acc: 0.7696\n",
      "Epoch 16/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3087 - acc: 0.7596 - val_loss: 0.3073 - val_acc: 0.7652\n",
      "Epoch 17/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3062 - acc: 0.7620 - val_loss: 0.3047 - val_acc: 0.7721\n",
      "Epoch 18/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3039 - acc: 0.7644 - val_loss: 0.3041 - val_acc: 0.7741\n",
      "Epoch 19/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.3028 - acc: 0.7650 - val_loss: 0.3019 - val_acc: 0.7706\n",
      "Epoch 20/50\n",
      "566006/566006 [==============================] - 149s 264us/step - loss: 0.3006 - acc: 0.7676 - val_loss: 0.3017 - val_acc: 0.7732\n",
      "Epoch 21/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.2991 - acc: 0.7693 - val_loss: 0.3030 - val_acc: 0.7762\n",
      "Epoch 22/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.2965 - acc: 0.7720 - val_loss: 0.2969 - val_acc: 0.7710\n",
      "Epoch 23/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2951 - acc: 0.7727 - val_loss: 0.2978 - val_acc: 0.7689\n",
      "Epoch 24/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2935 - acc: 0.7748 - val_loss: 0.2948 - val_acc: 0.7765\n",
      "Epoch 25/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2928 - acc: 0.7754 - val_loss: 0.2966 - val_acc: 0.7805\n",
      "Epoch 26/50\n",
      "566006/566006 [==============================] - 149s 262us/step - loss: 0.2906 - acc: 0.7780 - val_loss: 0.2965 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2893 - acc: 0.7794 - val_loss: 0.2970 - val_acc: 0.7876\n",
      "Epoch 28/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2880 - acc: 0.7805 - val_loss: 0.2914 - val_acc: 0.7789\n",
      "Epoch 29/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2871 - acc: 0.7819 - val_loss: 0.2941 - val_acc: 0.7836\n",
      "Epoch 30/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2854 - acc: 0.7831 - val_loss: 0.2952 - val_acc: 0.7853\n",
      "Epoch 31/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2847 - acc: 0.7834 - val_loss: 0.2944 - val_acc: 0.7897\n",
      "Epoch 32/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2837 - acc: 0.7847 - val_loss: 0.2917 - val_acc: 0.7826\n",
      "Epoch 33/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2823 - acc: 0.7860 - val_loss: 0.2927 - val_acc: 0.7870\n",
      "Epoch 34/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2814 - acc: 0.7874 - val_loss: 0.2921 - val_acc: 0.7827\n",
      "Epoch 35/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2798 - acc: 0.7882 - val_loss: 0.2903 - val_acc: 0.7873\n",
      "Epoch 36/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2798 - acc: 0.7889 - val_loss: 0.2895 - val_acc: 0.7833\n",
      "Epoch 37/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2786 - acc: 0.7901 - val_loss: 0.2891 - val_acc: 0.7873\n",
      "Epoch 38/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2775 - acc: 0.7911 - val_loss: 0.2899 - val_acc: 0.7897\n",
      "Epoch 39/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2765 - acc: 0.7918 - val_loss: 0.2878 - val_acc: 0.7871\n",
      "Epoch 40/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2755 - acc: 0.7928 - val_loss: 0.2884 - val_acc: 0.7892\n",
      "Epoch 41/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2753 - acc: 0.7927 - val_loss: 0.2868 - val_acc: 0.7906\n",
      "Epoch 42/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2738 - acc: 0.7941 - val_loss: 0.2894 - val_acc: 0.7953\n",
      "Epoch 43/50\n",
      "566006/566006 [==============================] - 149s 262us/step - loss: 0.2740 - acc: 0.7942 - val_loss: 0.2920 - val_acc: 0.7961\n",
      "Epoch 44/50\n",
      "566006/566006 [==============================] - 149s 262us/step - loss: 0.2730 - acc: 0.7954 - val_loss: 0.2844 - val_acc: 0.7860\n",
      "Epoch 45/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2722 - acc: 0.7961 - val_loss: 0.2910 - val_acc: 0.7963\n",
      "Epoch 46/50\n",
      "566006/566006 [==============================] - 149s 263us/step - loss: 0.2715 - acc: 0.7964 - val_loss: 0.2880 - val_acc: 0.7951\n",
      "Epoch 47/50\n",
      "566006/566006 [==============================] - 148s 261us/step - loss: 0.2707 - acc: 0.7974 - val_loss: 0.2894 - val_acc: 0.7981\n",
      "Epoch 48/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2704 - acc: 0.7977 - val_loss: 0.2869 - val_acc: 0.7960\n",
      "Epoch 49/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2695 - acc: 0.7989 - val_loss: 0.2838 - val_acc: 0.7889\n",
      "Epoch 50/50\n",
      "566006/566006 [==============================] - 148s 262us/step - loss: 0.2689 - acc: 0.7987 - val_loss: 0.2878 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model_att_before = Model(inputs=[sequence_1_input_att_before, sequence_2_input_att_before], outputs=preds_att_before)\n",
    "model_att_before.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model_att_before.summary()\n",
    "bst_model_path_att_before = 'att_before_' + STAMP + '.h5'\n",
    "model_checkpoint_att_before = ModelCheckpoint(bst_model_path_att_before, save_best_only=True, save_weights_only=True)\n",
    "if LOAD_DATA:\n",
    "    model_att_before.load_weights(bst_model_path_att_before)\n",
    "hist_att_before = model_att_before.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=50, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[model_checkpoint_att_before])\n",
    "model_att_before.load_weights(bst_model_path_att_before)\n",
    "bst_val_score_att_before = min(hist_att_before.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_att_after = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "lstm_layer_att_after = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "sequence_1_input_att_after = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1_att_after = embedding_layer_att_after(sequence_1_input_att_after)\n",
    "x1_att_after = lstm_layer_att_after(embedded_sequences_1_att_after)\n",
    "x1_att_after = add_attention_after_lstm(x1_att_after, SINGLE_ATTENTION_VECTOR=False)\n",
    "sequence_2_input_att_after = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2_att_after = embedding_layer_att_after(sequence_2_input_att_after)\n",
    "y1_att_after = lstm_layer_att_after(embedded_sequences_2_att_after)\n",
    "y1_att_after = add_attention_after_lstm(y1_att_after, SINGLE_ATTENTION_VECTOR=False)\n",
    "merged_att_after = concatenate([x1_att_after, y1_att_after])\n",
    "merged_att_after = Dropout(rate_drop_dense)(merged_att_after)\n",
    "merged_att_after = BatchNormalization()(merged_att_after)\n",
    "merged_att_after = Dense(num_dense, activation=act)(merged_att_after)\n",
    "merged_att_after = Dropout(rate_drop_dense)(merged_att_after)\n",
    "merged_att_after = BatchNormalization()(merged_att_after)\n",
    "preds_att_after = Dense(1, activation='sigmoid')(merged_att_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 30, 300)      25655700    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 265)          599960      embedding_3[0][0]                \n",
      "                                                                 embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 265)       0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 265)       0           lstm_3[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 265, 1)       0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, 265, 1)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 265, 1)       0           permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 265, 1)       0           permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 265, 1)       2           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 265, 1)       2           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, 1, 265)       0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 1, 265)       0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 265)          0           permute_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 265)          0           permute_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 265)          0           lstm_3[0][0]                     \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 265)          0           lstm_3[1][0]                     \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 530)          0           multiply_3[0][0]                 \n",
      "                                                                 multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 530)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 530)          2120        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 145)          76995       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 145)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 145)          580         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            146         batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 26,335,505\n",
      "Trainable params: 678,455\n",
      "Non-trainable params: 25,657,050\n",
      "__________________________________________________________________________________________________\n",
      "Train on 566006 samples, validate on 242574 samples\n",
      "Epoch 1/50\n",
      "566006/566006 [==============================] - 128s 227us/step - loss: 0.4397 - acc: 0.6695 - val_loss: 0.3685 - val_acc: 0.7189\n",
      "Epoch 2/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.3706 - acc: 0.6986 - val_loss: 0.3455 - val_acc: 0.7106\n",
      "Epoch 3/50\n",
      "566006/566006 [==============================] - 125s 222us/step - loss: 0.3497 - acc: 0.7161 - val_loss: 0.3385 - val_acc: 0.7361\n",
      "Epoch 4/50\n",
      "566006/566006 [==============================] - 125s 222us/step - loss: 0.3340 - acc: 0.7302 - val_loss: 0.3213 - val_acc: 0.7507\n",
      "Epoch 5/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.3218 - acc: 0.7420 - val_loss: 0.3129 - val_acc: 0.7648\n",
      "Epoch 6/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.3126 - acc: 0.7511 - val_loss: 0.3013 - val_acc: 0.7669\n",
      "Epoch 7/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.3040 - acc: 0.7600 - val_loss: 0.2993 - val_acc: 0.7708\n",
      "Epoch 8/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2971 - acc: 0.7661 - val_loss: 0.2948 - val_acc: 0.7887\n",
      "Epoch 9/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2906 - acc: 0.7727 - val_loss: 0.2865 - val_acc: 0.7813\n",
      "Epoch 10/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2847 - acc: 0.7791 - val_loss: 0.2880 - val_acc: 0.7905\n",
      "Epoch 11/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2795 - acc: 0.7840 - val_loss: 0.2888 - val_acc: 0.8002\n",
      "Epoch 12/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2747 - acc: 0.7886 - val_loss: 0.2806 - val_acc: 0.7989\n",
      "Epoch 13/50\n",
      "566006/566006 [==============================] - 125s 222us/step - loss: 0.2704 - acc: 0.7932 - val_loss: 0.2813 - val_acc: 0.8016\n",
      "Epoch 14/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2665 - acc: 0.7970 - val_loss: 0.2802 - val_acc: 0.8041\n",
      "Epoch 15/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2628 - acc: 0.8008 - val_loss: 0.2850 - val_acc: 0.8078\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2593 - acc: 0.8041 - val_loss: 0.2782 - val_acc: 0.8044\n",
      "Epoch 17/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2562 - acc: 0.8069 - val_loss: 0.2799 - val_acc: 0.8113\n",
      "Epoch 18/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2535 - acc: 0.8093 - val_loss: 0.2770 - val_acc: 0.8024\n",
      "Epoch 19/50\n",
      "566006/566006 [==============================] - 125s 222us/step - loss: 0.2506 - acc: 0.8120 - val_loss: 0.2770 - val_acc: 0.8114\n",
      "Epoch 20/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2482 - acc: 0.8146 - val_loss: 0.2805 - val_acc: 0.8153\n",
      "Epoch 21/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2467 - acc: 0.8157 - val_loss: 0.2833 - val_acc: 0.8154\n",
      "Epoch 22/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2442 - acc: 0.8183 - val_loss: 0.2794 - val_acc: 0.8173\n",
      "Epoch 23/50\n",
      "566006/566006 [==============================] - 125s 221us/step - loss: 0.2428 - acc: 0.8200 - val_loss: 0.2805 - val_acc: 0.8167\n",
      "Epoch 24/50\n",
      "566006/566006 [==============================] - 124s 220us/step - loss: 0.2406 - acc: 0.8215 - val_loss: 0.2759 - val_acc: 0.8159\n",
      "Epoch 25/50\n",
      "566006/566006 [==============================] - 125s 221us/step - loss: 0.2393 - acc: 0.8234 - val_loss: 0.2812 - val_acc: 0.8205\n",
      "Epoch 26/50\n",
      "566006/566006 [==============================] - 142s 250us/step - loss: 0.2371 - acc: 0.8248 - val_loss: 0.2772 - val_acc: 0.8166\n",
      "Epoch 27/50\n",
      "566006/566006 [==============================] - 188s 333us/step - loss: 0.2356 - acc: 0.8267 - val_loss: 0.2801 - val_acc: 0.8193\n",
      "Epoch 28/50\n",
      "566006/566006 [==============================] - 151s 267us/step - loss: 0.2343 - acc: 0.8279 - val_loss: 0.2768 - val_acc: 0.8182\n",
      "Epoch 29/50\n",
      "566006/566006 [==============================] - 153s 270us/step - loss: 0.2332 - acc: 0.8287 - val_loss: 0.2738 - val_acc: 0.8187\n",
      "Epoch 30/50\n",
      "566006/566006 [==============================] - 153s 271us/step - loss: 0.2321 - acc: 0.8297 - val_loss: 0.2843 - val_acc: 0.8248\n",
      "Epoch 31/50\n",
      "566006/566006 [==============================] - 153s 270us/step - loss: 0.2301 - acc: 0.8314 - val_loss: 0.2779 - val_acc: 0.8214\n",
      "Epoch 32/50\n",
      "566006/566006 [==============================] - 155s 273us/step - loss: 0.2294 - acc: 0.8323 - val_loss: 0.2760 - val_acc: 0.8206\n",
      "Epoch 33/50\n",
      "566006/566006 [==============================] - 153s 270us/step - loss: 0.2289 - acc: 0.8330 - val_loss: 0.2769 - val_acc: 0.8220\n",
      "Epoch 34/50\n",
      "566006/566006 [==============================] - 150s 265us/step - loss: 0.2270 - acc: 0.8343 - val_loss: 0.2778 - val_acc: 0.8226\n",
      "Epoch 35/50\n",
      "566006/566006 [==============================] - 152s 269us/step - loss: 0.2259 - acc: 0.8354 - val_loss: 0.2751 - val_acc: 0.8231\n",
      "Epoch 36/50\n",
      "566006/566006 [==============================] - 157s 277us/step - loss: 0.2253 - acc: 0.8364 - val_loss: 0.2775 - val_acc: 0.8244\n",
      "Epoch 37/50\n",
      "566006/566006 [==============================] - 154s 272us/step - loss: 0.2242 - acc: 0.8371 - val_loss: 0.2737 - val_acc: 0.8224\n",
      "Epoch 38/50\n",
      "566006/566006 [==============================] - 153s 270us/step - loss: 0.2240 - acc: 0.8376 - val_loss: 0.2779 - val_acc: 0.8217\n",
      "Epoch 39/50\n",
      "566006/566006 [==============================] - 151s 267us/step - loss: 0.2232 - acc: 0.8379 - val_loss: 0.2809 - val_acc: 0.8240\n",
      "Epoch 40/50\n",
      "566006/566006 [==============================] - 157s 277us/step - loss: 0.2217 - acc: 0.8393 - val_loss: 0.2823 - val_acc: 0.8278\n",
      "Epoch 41/50\n",
      "566006/566006 [==============================] - 130s 229us/step - loss: 0.2214 - acc: 0.8399 - val_loss: 0.2799 - val_acc: 0.8243\n",
      "Epoch 42/50\n",
      "566006/566006 [==============================] - 125s 221us/step - loss: 0.2204 - acc: 0.8410 - val_loss: 0.2841 - val_acc: 0.8271\n",
      "Epoch 43/50\n",
      "566006/566006 [==============================] - 125s 221us/step - loss: 0.2195 - acc: 0.8415 - val_loss: 0.2750 - val_acc: 0.8242\n",
      "Epoch 44/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2194 - acc: 0.8415 - val_loss: 0.2755 - val_acc: 0.8260\n",
      "Epoch 45/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2184 - acc: 0.8427 - val_loss: 0.2771 - val_acc: 0.8239\n",
      "Epoch 46/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2176 - acc: 0.8434 - val_loss: 0.2789 - val_acc: 0.8277\n",
      "Epoch 47/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2177 - acc: 0.8430 - val_loss: 0.2748 - val_acc: 0.8272\n",
      "Epoch 48/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2167 - acc: 0.8440 - val_loss: 0.2734 - val_acc: 0.8253\n",
      "Epoch 49/50\n",
      "566006/566006 [==============================] - 125s 222us/step - loss: 0.2156 - acc: 0.8448 - val_loss: 0.2840 - val_acc: 0.8311\n",
      "Epoch 50/50\n",
      "566006/566006 [==============================] - 126s 222us/step - loss: 0.2161 - acc: 0.8448 - val_loss: 0.2822 - val_acc: 0.8280\n"
     ]
    }
   ],
   "source": [
    "model_att_after = Model(inputs=[sequence_1_input_att_after, sequence_2_input_att_after], outputs=preds_att_after)\n",
    "model_att_after.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model_att_after.summary()\n",
    "bst_model_path_att_after = 'att_after_' + STAMP + '.h5'\n",
    "model_checkpoint_att_after = ModelCheckpoint(bst_model_path_att_after, save_best_only=True, save_weights_only=True)\n",
    "if LOAD_DATA:\n",
    "    model_att_after.load_weights(bst_model_path_att_after)\n",
    "hist_att_after = model_att_after.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=50, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[model_checkpoint_att_after])\n",
    "model_att_after.load_weights(bst_model_path_att_after)\n",
    "bst_val_score_att_after = min(hist_att_after.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
