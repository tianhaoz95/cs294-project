{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianh\\Desktop\\environments\\mlenv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\tianh\\Desktop\\environments\\mlenv\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import timeit\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.layers as KL\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import backend as KB\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Reshape, Permute, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.3\n",
    "TIME_STEPS = 200\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_AFTER_LSTM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "STAMP = 'lstm_baseline_cnn_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 404290 texts in train.csv\n",
      "Found 85518 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 37391\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n"
     ]
    }
   ],
   "source": [
    "train_orig =  pd.read_csv(TRAIN_DATA_FILE, header=0)\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "train_cp = train_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "train_cp['q1_hash'] = train_cp['question1'].map(questions_dict)\n",
    "train_cp['q2_hash'] = train_cp['question2'].map(questions_dict)\n",
    "q1_vc = train_cp.q1_hash.value_counts().to_dict()\n",
    "q2_vc = train_cp.q2_hash.value_counts().to_dict()\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "train_cp['q1_freq'] = train_cp['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "train_cp['q2_freq'] = train_cp['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "train_comb = train_cp[train_cp['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "corr_mat = train_comb.corr()\n",
    "# corr_mat.head()\n",
    "print(train_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=30,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(30,))\n",
    "    seq2 = Input(shape=(30,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: KB.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "#     # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "#     # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "#     # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "#     distance_input = Input(shape=(20,))\n",
    "#     distance_dense = BatchNormalization()(distance_input)\n",
    "#     distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "    merge = concatenate([diff, mul])\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_conv1D_(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_baseline_cnn_261_139_0.16_0.29\n",
      "Train on 566006 samples, validate on 242574 samples\n",
      "Epoch 1/50\n",
      "566006/566006 [==============================] - 65s 114us/step - loss: 0.3748 - acc: 0.7434 - val_loss: 0.2880 - val_acc: 0.7641\n",
      "Epoch 2/50\n",
      "566006/566006 [==============================] - 61s 109us/step - loss: 0.2519 - acc: 0.8186 - val_loss: 0.2754 - val_acc: 0.7868\n",
      "Epoch 3/50\n",
      "566006/566006 [==============================] - 61s 109us/step - loss: 0.2068 - acc: 0.8575 - val_loss: 0.2778 - val_acc: 0.8336\n",
      "Epoch 4/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.1741 - acc: 0.8839 - val_loss: 0.2732 - val_acc: 0.8223\n",
      "Epoch 5/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.1496 - acc: 0.9023 - val_loss: 0.2884 - val_acc: 0.8438\n",
      "Epoch 6/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.1314 - acc: 0.9158 - val_loss: 0.2810 - val_acc: 0.8334\n",
      "Epoch 7/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.1164 - acc: 0.9263 - val_loss: 0.3087 - val_acc: 0.8448\n",
      "Epoch 8/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.1068 - acc: 0.9332 - val_loss: 0.3102 - val_acc: 0.8392\n",
      "Epoch 9/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0970 - acc: 0.9399 - val_loss: 0.3247 - val_acc: 0.8440\n",
      "Epoch 10/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0893 - acc: 0.9455 - val_loss: 0.3394 - val_acc: 0.8486\n",
      "Epoch 11/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0835 - acc: 0.9494 - val_loss: 0.3352 - val_acc: 0.8469\n",
      "Epoch 12/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0787 - acc: 0.9525 - val_loss: 0.3311 - val_acc: 0.8471\n",
      "Epoch 13/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0731 - acc: 0.9562 - val_loss: 0.3357 - val_acc: 0.8459\n",
      "Epoch 14/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0702 - acc: 0.9585 - val_loss: 0.3543 - val_acc: 0.8490\n",
      "Epoch 15/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0668 - acc: 0.9608 - val_loss: 0.3578 - val_acc: 0.8516\n",
      "Epoch 16/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0638 - acc: 0.9622 - val_loss: 0.3695 - val_acc: 0.8551\n",
      "Epoch 17/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0618 - acc: 0.9638 - val_loss: 0.3592 - val_acc: 0.8516\n",
      "Epoch 18/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0594 - acc: 0.9652 - val_loss: 0.3749 - val_acc: 0.8513\n",
      "Epoch 19/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0570 - acc: 0.9668 - val_loss: 0.3998 - val_acc: 0.8574\n",
      "Epoch 20/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0551 - acc: 0.9680 - val_loss: 0.3865 - val_acc: 0.8575\n",
      "Epoch 21/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0528 - acc: 0.9694 - val_loss: 0.3755 - val_acc: 0.8533\n",
      "Epoch 22/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0520 - acc: 0.9700 - val_loss: 0.3740 - val_acc: 0.8523\n",
      "Epoch 23/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0503 - acc: 0.9712 - val_loss: 0.3929 - val_acc: 0.8554\n",
      "Epoch 24/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0492 - acc: 0.9719 - val_loss: 0.4165 - val_acc: 0.8599\n",
      "Epoch 25/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0473 - acc: 0.9732 - val_loss: 0.4167 - val_acc: 0.8588\n",
      "Epoch 26/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0463 - acc: 0.9737 - val_loss: 0.3932 - val_acc: 0.8574\n",
      "Epoch 27/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0454 - acc: 0.9742 - val_loss: 0.4018 - val_acc: 0.8546\n",
      "Epoch 28/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0447 - acc: 0.9749 - val_loss: 0.4403 - val_acc: 0.8594\n",
      "Epoch 29/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0431 - acc: 0.9757 - val_loss: 0.3843 - val_acc: 0.8541\n",
      "Epoch 30/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0427 - acc: 0.9761 - val_loss: 0.4344 - val_acc: 0.8595\n",
      "Epoch 31/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0409 - acc: 0.9766 - val_loss: 0.4272 - val_acc: 0.8599\n",
      "Epoch 32/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0414 - acc: 0.9770 - val_loss: 0.3977 - val_acc: 0.8539\n",
      "Epoch 33/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0401 - acc: 0.9776 - val_loss: 0.3888 - val_acc: 0.8524\n",
      "Epoch 34/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0395 - acc: 0.9781 - val_loss: 0.4109 - val_acc: 0.8594\n",
      "Epoch 35/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0371 - acc: 0.9794 - val_loss: 0.4387 - val_acc: 0.8589\n",
      "Epoch 36/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0378 - acc: 0.9790 - val_loss: 0.4105 - val_acc: 0.8566\n",
      "Epoch 37/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0373 - acc: 0.9794 - val_loss: 0.4149 - val_acc: 0.8592\n",
      "Epoch 38/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0366 - acc: 0.9796 - val_loss: 0.4279 - val_acc: 0.8591\n",
      "Epoch 39/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0359 - acc: 0.9803 - val_loss: 0.4127 - val_acc: 0.8575\n",
      "Epoch 40/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0344 - acc: 0.9809 - val_loss: 0.4482 - val_acc: 0.8615\n",
      "Epoch 41/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0348 - acc: 0.9809 - val_loss: 0.4303 - val_acc: 0.8592\n",
      "Epoch 42/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0342 - acc: 0.9812 - val_loss: 0.4335 - val_acc: 0.8609\n",
      "Epoch 43/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0345 - acc: 0.9811 - val_loss: 0.4249 - val_acc: 0.8608\n",
      "Epoch 44/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0333 - acc: 0.9817 - val_loss: 0.4159 - val_acc: 0.8600\n",
      "Epoch 45/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0327 - acc: 0.9821 - val_loss: 0.4467 - val_acc: 0.8577\n",
      "Epoch 46/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0328 - acc: 0.9818 - val_loss: 0.4379 - val_acc: 0.8596\n",
      "Epoch 47/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0309 - acc: 0.9831 - val_loss: 0.4229 - val_acc: 0.8589\n",
      "Epoch 48/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0308 - acc: 0.9831 - val_loss: 0.4246 - val_acc: 0.8597\n",
      "Epoch 49/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0310 - acc: 0.9830 - val_loss: 0.4316 - val_acc: 0.8583\n",
      "Epoch 50/50\n",
      "566006/566006 [==============================] - 62s 109us/step - loss: 0.0307 - acc: 0.9832 - val_loss: 0.4402 - val_acc: 0.8595\n"
     ]
    }
   ],
   "source": [
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=50, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
